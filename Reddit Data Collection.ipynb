{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839ec9e3",
   "metadata": {},
   "source": [
    "# Reddit Data Collection\n",
    "\n",
    "The goal is to collect posts from Reddit that mirror our Twitter data collection using the [PushShift API](https://github.com/pushshift/api). To that end we can create a function that will walk through search results for posts that match a given query defined by the kwargs that are passed in, and a start/end time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "626182cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def search_pushshift(start, end, max_errors=30, sleep=1, **kwargs):\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    \n",
    "    params = kwargs\n",
    "    params['results'] = 500\n",
    "        \n",
    "    now = datetime.now()\n",
    "    hour = int((now - end).total_seconds() / (60 * 60))\n",
    "    num_hours = int((now - start).total_seconds() / (60 * 60))\n",
    "    step = 1\n",
    "    errs = 0\n",
    "\n",
    "    while hour < num_hours:\n",
    "        params['before'] = f'{hour}h'\n",
    "        params['after'] = f'{hour + step}h'\n",
    "        try:\n",
    "            resp = requests.get(url, params=params)\n",
    "            if resp.status_code != 200:\n",
    "                errs += 1\n",
    "            else:\n",
    "                errs = 0\n",
    "                results = resp.json()['data']\n",
    "                if len(results) > 0: \n",
    "                    for result in results:\n",
    "                        result['created'] = datetime.fromtimestamp(result['created_utc'])\n",
    "                        if result['created'] < start:\n",
    "                            break                            \n",
    "                        yield result\n",
    "                    hour += step\n",
    "                    step = 1            \n",
    "                else:\n",
    "                    step = 2 * step\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f'got exception: {e}')\n",
    "            errs += 1\n",
    "            \n",
    "        if errs > max_errors:\n",
    "            print(f'bailing after {max_errors} consecutive errors')\n",
    "            \n",
    "        time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb6073b",
   "metadata": {},
   "source": [
    "So for example you can search for \"police violence\" posts from 2021-01-01 to 2021-01-04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9b88de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejmjlz 2020-01-03 22:01:59 Imran Khan Tweets Indian Police Pogrom On Muslims\n",
      "eji17g 2020-01-03 16:49:16 [IN] - Videos of police violence surface in Bihar | The Hindu\n",
      "ejhke1 2020-01-03 16:16:32 Pakistan's Prime Minister Imran Khan Tweets Fake Video From Bangladesh, Tries To Pass It Off As Police Violence In India.\n",
      "eib2oo 2020-01-01 00:05:44 35 [M4F] Seattle/Online - Nerdy socialist viking seeks SPICY chats and fun times\n",
      "eid6ya 2020-01-01 03:14:21 r/selfawarewolves advocating for anti-police violence \"cops should be subject to abuse everywhere they go. They’ve earned it\" (100+ upvotes)\n"
     ]
    }
   ],
   "source": [
    "for result in search_pushshift(q='\"police violence\"', start=datetime(2020, 1, 1), end=datetime(2020, 1, 4)):\n",
    "    print(result['id'], result['created'], result['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd97fc",
   "metadata": {},
   "source": [
    "Now we can try the same but searching only the title of the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bb968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eji17g 2020-01-03 16:49:16 [IN] - Videos of police violence surface in Bihar | The Hindu\n",
      "ejhke1 2020-01-03 16:16:32 Pakistan's Prime Minister Imran Khan Tweets Fake Video From Bangladesh, Tries To Pass It Off As Police Violence In India.\n",
      "eid6ya 2020-01-01 03:14:21 r/selfawarewolves advocating for anti-police violence \"cops should be subject to abuse everywhere they go. They’ve earned it\" (100+ upvotes)\n"
     ]
    }
   ],
   "source": [
    "for result in search_pushshift(title='\"police violence\"', start=datetime(2020, 1, 1), end=datetime(2020, 1, 4)):\n",
    "    print(result['id'], result['created'], result['title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7779a50",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Experimentation has shown that the following properties come back from the PushShift API. We can use them to create a CSV where each row is a submission and each column is a property of that submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5169e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['pinned', 'secure_media_embed', 'post_hint', 'parent_whitelist_status', 'author_premium', 'pwls', 'all_awardings', 'author_flair_richtext', 'whitelist_status', 'gildings', 'media_metadata', 'score', 'author_cakeday', 'thumbnail_height', 'treatment_tags', 'created', 'media_embed', 'is_crosspostable', 'upvote_ratio', 'is_gallery', 'author_flair_template_id', 'spoiler', 'secure_media', 'crosspost_parent_list', 'url_overridden_by_dest', 'allow_live_comments', 'subreddit_id', 'link_flair_css_class', 'url', 'can_mod_post', 'author_flair_text', 'media', 'domain', 'preview', 'wls', 'author_flair_type', 'is_original_content', 'locked', 'removed_by_category', 'thumbnail_width', 'permalink', 'awarders', 'suggested_sort', 'link_flair_background_color', 'content_categories', 'link_flair_text_color', 'selftext', 'thumbnail', 'link_flair_type', 'author_flair_background_color', 'is_robot_indexable', 'media_only', 'crosspost_parent', 'over_18', 'author_patreon_flair', 'total_awards_received', 'author_flair_text_color', 'author_fullname', 'contest_mode', 'gallery_data', 'id', 'num_comments', 'retrieved_on', 'is_video', 'send_replies', 'is_self', 'author', 'subreddit', 'stickied', 'full_link', 'no_follow', 'poll_data', 'author_flair_css_class', 'subreddit_type', 'is_reddit_media_domain', 'num_crossposts', 'link_flair_template_id', 'is_meta', 'subreddit_subscribers', 'created_utc', 'link_flair_text', 'link_flair_richtext', 'title', 'edited', 'banned_by', 'author_cakeday', 'rpan_video', 'event_start', 'event_enshowsd', 'collections', 'steward_reports', 'discussion_type', 'gilded', 'event_is_live', 'edited', 'banned_by', 'author_cakeday', 'rpan_video', 'event_start', 'event_enshowsd', 'collections', 'steward_reports', 'discussion_type', 'gilded', 'event_is_live']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41699a02-0a73-4343-b778-3399cad2ad5a",
   "metadata": {},
   "source": [
    "Now we are ready to do the data collection for the following datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87eee18-e00b-46f1-aeaa-7d27b8548e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    '\"black people\"',\n",
    "    '\"racial wealth gap\"',\n",
    "    'racism'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv \n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path('../data/reddit.pull')\n",
    "\n",
    "def collect(query):\n",
    "    norm_q = query.replace(' ', '_').replace('\"', '')\n",
    "    csv_file = data_dir / f'{norm_q}.csv'\n",
    "    print()\n",
    "    print(csv_file)\n",
    "    with csv_file.open('w') as fh:\n",
    "        out = csv.DictWriter(fh, fieldnames=cols, extrasaction='ignore')\n",
    "        out.writeheader()\n",
    "        for result in search_pushshift(title=query, start=datetime(2020, 5, 25), end=datetime(2021, 5, 25), sleep=1):\n",
    "            out.writerow(result)\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "for query in queries:\n",
    "    collect(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f97c1",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "Since we are analyzing conversations it's important to get the comments for these submissions as well.PushShift have an API to get the comment IDs for a post ID, e.g. \n",
    "\n",
    "https://api.pushshift.io/reddit/submission/comment_ids/pe65v4\n",
    "\n",
    "And then they have an API to get the comments themselves using the discovered comment ids:\n",
    "\n",
    "https://api.pushshift.io/reddit/comment/search?ids=hav4td5,havo62q\n",
    "\n",
    "The function `get_comments()` will take a post id and return a list of comment objects for that post. Since there are lots of comments for some posts we need to be careful to only ask for 50 comment ids at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1e4c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "        \n",
    "def get_comments(post_id, sleep=.5):\n",
    "    time.sleep(sleep)\n",
    "    comment_ids = requests.get(f'https://api.pushshift.io/reddit/submission/comment_ids/{post_id}').json()['data']\n",
    "    for ids in grouper(50, comment_ids):\n",
    "        resp = requests.get(f'https://api.pushshift.io/reddit/comment/search?ids={\",\".join(ids)}')\n",
    "        if resp.status_code == 200:\n",
    "            yield from resp.json()['data']\n",
    "        else:\n",
    "            print(f'error: {resp}')\n",
    "            return\n",
    "        \n",
    "# for chunking an iteratorinto tuples of size n\n",
    "def grouper(n, iterable):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab502b",
   "metadata": {},
   "source": [
    "Lets try to get the first comment for post id [pe65v4](https://www.reddit.com/r/pushshift/comments/pe65v4/update_on_reddit_monthly_comment_dumps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e344dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"all_awardings\": [],\n",
      "  \"approved_at_utc\": null,\n",
      "  \"associated_award\": null,\n",
      "  \"author\": \"Watchful1\",\n",
      "  \"author_flair_background_color\": null,\n",
      "  \"author_flair_css_class\": null,\n",
      "  \"author_flair_richtext\": [],\n",
      "  \"author_flair_template_id\": null,\n",
      "  \"author_flair_text\": null,\n",
      "  \"author_flair_text_color\": null,\n",
      "  \"author_flair_type\": \"text\",\n",
      "  \"author_fullname\": \"t2_d0z23\",\n",
      "  \"author_is_blocked\": false,\n",
      "  \"author_patreon_flair\": false,\n",
      "  \"author_premium\": true,\n",
      "  \"awarders\": [],\n",
      "  \"banned_at_utc\": null,\n",
      "  \"body\": \"Thanks! I've been looking forward to them.\\n\\nWill this include the recompressed older files?\",\n",
      "  \"can_mod_post\": false,\n",
      "  \"collapsed\": false,\n",
      "  \"collapsed_because_crowd_control\": null,\n",
      "  \"collapsed_reason\": null,\n",
      "  \"collapsed_reason_code\": null,\n",
      "  \"comment_type\": null,\n",
      "  \"created_utc\": 1630279831,\n",
      "  \"distinguished\": null,\n",
      "  \"edited\": false,\n",
      "  \"gildings\": {},\n",
      "  \"id\": \"hav4td5\",\n",
      "  \"is_submitter\": false,\n",
      "  \"link_id\": \"t3_pe65v4\",\n",
      "  \"locked\": false,\n",
      "  \"no_follow\": true,\n",
      "  \"parent_id\": \"t3_pe65v4\",\n",
      "  \"permalink\": \"/r/pushshift/comments/pe65v4/update_on_reddit_monthly_comment_dumps/hav4td5/\",\n",
      "  \"retrieved_on\": 1630363272,\n",
      "  \"score\": 1,\n",
      "  \"send_replies\": true,\n",
      "  \"stickied\": false,\n",
      "  \"subreddit\": \"pushshift\",\n",
      "  \"subreddit_id\": \"t5_37z6f\",\n",
      "  \"top_awarded_type\": null,\n",
      "  \"total_awards_received\": 0,\n",
      "  \"treatment_tags\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "comment = next(get_comments('pe65v4'))\n",
    "print(json.dumps(comment, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bebfb95-6290-4d2f-96ee-f6f436a1a312",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "For analysis its easiest if the conversations are converted to CSV and stored alongside the posts that they are from. Unlike the Twitter conversations we aren't constrained by an API quota, so we can attempt to get all the conversation threads.\n",
    "\n",
    "The properties of the comment JSON object we just retrieved can be used as the columns of our CSV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb54268f-3dab-435d-8deb-7c7cab0268ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_cols = list(comment.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e30a4-edc9-4325-a7a8-8bb4d188d4fa",
   "metadata": {},
   "source": [
    "The function `save_comments()` will read a file of search results and retrieve comments for the posts that have comments. Experimentation has shown that a large value in one of the rows requires csv to be instructed to load larger cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7925f4c-d887-4f7a-9ebe-2ede481da97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit = 1131072\n",
    "\n",
    "def save_comments(posts_file):\n",
    "    print(f\"\\nprocessing {posts_file}\")\n",
    "    convs_dir = data_dir / (posts_file.stem + \"_convs\")\n",
    "    \n",
    "    if not convs_dir.is_dir():\n",
    "        convs_dir.mkdir()\n",
    "        \n",
    "    for post in csv.DictReader(posts_file.open()):\n",
    "        \n",
    "        if int(post['num_comments']) > 0:\n",
    "            \n",
    "            sys.stdout.write(f'\\n{post[\"id\"]}[{post[\"num_comments\"]}]')\n",
    "            csv_file = convs_dir / f\"{post['id']}.csv\"\n",
    "            \n",
    "            with csv_file.open('w') as fh: \n",
    "                out = csv.DictWriter(fh, fieldnames=comment_cols, extrasaction='ignore')\n",
    "                out.writeheader()\n",
    "                for comment in get_comments(post['id']):\n",
    "                    out.writerow(comment)\n",
    "                    sys.stdout.write('.')\n",
    "                    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b468622-1261-4511-ac7c-c51550ed56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post_file in data_dir.glob('reddit_*.csv'):\n",
    "    save_comments(post_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990f1bc-f969-418d-a75b-e78caace908d",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Since the Twitter conversations were sampled we can randomly sample the Reddit ones too. Since we have `num_comments` we can use it to sample conversations that have more than 5 comments. We oversample since some threads no longer contain any comments (due to deletions or other discrepencies in the PushShift API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "86f6dc6b-22a8-49bc-9f93-898c2eecb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sh\n",
    "import pandas\n",
    "\n",
    "def sample(posts_file, n=30):\n",
    "    sample_dir = data_dir / f'{posts_file.stem}_{n}'\n",
    "    zip_file = data_dir / f'{posts_file.stem}_{n}.zip'\n",
    "\n",
    "    if zip_file.is_file():\n",
    "        print(f'already sampled {posts_file}')\n",
    "        return\n",
    "        \n",
    "    sh.mkdir(sample_dir)\n",
    "        \n",
    "    df = pandas.read_csv(posts_file)\n",
    "    df = df[df['num_comments'] >= 5]\n",
    "    \n",
    "    # we sample 10 extra  posts in case we hit some posts that lack comments\n",
    "    # this shouldn't happen because we've filtered on num_comments, but it does (deletes)\n",
    "    s = df.sample(n + 5)\n",
    "    \n",
    "    convs_found = 0\n",
    "    for post_id in s['id']:\n",
    "        comments_found = 0\n",
    "        comments_file = sample_dir / f'{post_id}.csv'\n",
    "        \n",
    "        with comments_file.open('w') as fh:\n",
    "            out = csv.DictWriter(fh, fieldnames=comment_cols, extrasaction='ignore')\n",
    "            out.writeheader()\n",
    "            for comment in get_comments(post_id):\n",
    "                comments_found += 1\n",
    "                out.writerow(comment)\n",
    "        \n",
    "        if comments_found >= 5:\n",
    "            print(posts_file, post_id, comments_found)\n",
    "            convs_found += 1\n",
    "        else:\n",
    "            sh.rm(comments_file)\n",
    "        \n",
    "        if convs_found >= n:\n",
    "            break\n",
    "    \n",
    "    pwd = os.getcwd()\n",
    "    os.chdir(data_dir)\n",
    "    sh.zip('-r', zip_file.name, sample_dir.name)\n",
    "    os.chdir(pwd)\n",
    "    \n",
    "    sh.rm('-rf', sample_dir)\n",
    "    print(f'created sample {zip_file}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1de59b66-f01d-41f6-b17f-b8ba17b533bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already sampled ../data/reddit.pull/reddit_black_people.csv\n",
      "already sampled ../data/reddit.pull/reddit_racial_wealth_gap.csv\n",
      "already sampled ../data/reddit.pull/reddit_racism.csv\n"
     ]
    }
   ],
   "source": [
    "for posts_csv in data_dir.glob('*.csv'):\n",
    "    sample(posts_csv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba3b64-ae67-4a40-a9d5-70e300fe07aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
