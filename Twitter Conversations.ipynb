{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e389e3-c058-4328-b0cb-3d01c4717e67",
   "metadata": {},
   "source": [
    "# Twitter Conversations\n",
    "\n",
    "This notebook analyzes previously collected tweets to identify *conversations* of interest, and to collect the conversations more throughly for closer analysis. In a previous set of notebooks stored in the s3://api.pull.code notebook a series of tweet JSON datasets were created using keyword searching and the Twitter Search API. These datasets were stored in the `s3://json.terms.files` bucket as a set of files: one file per search criteria. With the s3://json.terms.files bucket mounted this notebook will walk through the tweets and identify chatty conversations.\n",
    "\n",
    "## Chattiness\n",
    "\n",
    "This research is specifically about *bystander interventions* in social media. Part of the argument here is that a bystander intervention in public Twitter manifests as a conversation thread where two (or more) users are engaged in what looks like a conversation. In order to identifiy these threads a measure of *chattiness* will be generated using two pieces of information available in each tweet:\n",
    "\n",
    "* the `.public_metrics.reply_count` value available for each tweet, which is count of how many times a tweet has been replied to\n",
    "* the `.conversation_id` value which is an identifier for each threaded conversation\n",
    "\n",
    "In practice the number of users who participate in a thread is important too (a bystander intervention can't just be a user creating their own thread with no interaction). But we won't be able to ascertain that until we fetch the complete thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61546e67-b61a-4d43-adbe-50899cf6d38c",
   "metadata": {},
   "source": [
    "## Conversation Events\n",
    "\n",
    "The first step is to extract *conversation events* from the data retrieved from the Twitter Search API. `get_conversation_events()` takes a Path as an argument, and generates a conversation activity objects, each represented as a dictionary with the following keys: `author_id`, `conversation_id`, and `reply_count`. It filters out any retweets, which are important signals, but are not directly relevant to identifying conversation threads and bystander interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "13ce3efa-cce9-4269-82de-a4cb225668e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_conv_events(tweets_file):\n",
    "    \n",
    "    # parse each line of json\n",
    "    for line in tweets_file.open():\n",
    "        response = json.loads(line)\n",
    "        \n",
    "        # some reponses don't have data sometimes\n",
    "        if 'data' not in response:\n",
    "            continue\n",
    "        \n",
    "        # iterate through each tweets and yield any conversation info\n",
    "        for tweet in response['data']:      \n",
    "            \n",
    "            # ignore retweets\n",
    "            if 'retweeted' in [ref['type'] for ref in tweet.get('referenced_tweets', [])]:\n",
    "                continue\n",
    "            \n",
    "            # if the tweet has been replied to it's an event!\n",
    "            if tweet['public_metrics']['reply_count'] > 0:\n",
    "                yield({\n",
    "                    'conversation_id': tweet['conversation_id'],\n",
    "                    'reply_count': tweet['public_metrics']['reply_count']\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cf928-1a2d-4095-b9e2-37c31ce64d6d",
   "metadata": {},
   "source": [
    "Lets test it out just looking at the first 10 or so results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7dcc767d-e020-4c96-b89c-f7deda975eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_id': '1396979032340717571', 'reply_count': 3}\n",
      "{'conversation_id': '1396968282972905476', 'reply_count': 1}\n",
      "{'conversation_id': '1396958637080391680', 'reply_count': 1}\n",
      "{'conversation_id': '1396953594616717316', 'reply_count': 1}\n",
      "{'conversation_id': '1396951539785285635', 'reply_count': 4}\n",
      "{'conversation_id': '1396950852452167680', 'reply_count': 1}\n",
      "{'conversation_id': '1396950478324371457', 'reply_count': 1}\n",
      "{'conversation_id': '1396938780330938368', 'reply_count': 19}\n",
      "{'conversation_id': '1396866492625563651', 'reply_count': 1}\n",
      "{'conversation_id': '1396866900236423170', 'reply_count': 1}\n",
      "{'conversation_id': '1396861451776708610', 'reply_count': 4}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "count = 0\n",
    "for event in get_conv_events(Path('/home/ubuntu/jupyter/data/json.terms.files/prison_pipe_achievement_gap.json')):\n",
    "    print(event)\n",
    "    \n",
    "    # stop after 10\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43cb2c-8520-4348-b740-476c63e2b344",
   "metadata": {},
   "source": [
    "## Aggregate Conversations\n",
    "\n",
    "Next we need to aggregate the conversations by ID. `get_convs()` reads in the conversation events and generates a list of conversations that includes their: `conversation_id` and total `reply_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c25a46c5-e9e6-42b8-a6c9-fdfd19c3bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convs(events):\n",
    "    convos = {}\n",
    "    for e in events:\n",
    "        conv_id = e['conversation_id']\n",
    "        if conv_id in convos:\n",
    "            convos[conv_id]['reply_count'] += e['reply_count']\n",
    "        else:\n",
    "            convos[conv_id] = {\n",
    "                'conversation_id': conv_id,\n",
    "                'reply_count': e['reply_count'],\n",
    "            }\n",
    "    \n",
    "    # return the sorted conversations\n",
    "    convos = convos.values()\n",
    "    return sorted(convos, key=lambda c: c['reply_count'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c86d52-43bb-4dbd-ba06-10452b42a579",
   "metadata": {},
   "source": [
    "We can test this one too, by looking at the first 10 conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34d111e5-0ca3-44fb-9eed-fc297e765c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_id': '1271571630045696001', 'reply_count': 735}\n",
      "{'conversation_id': '1289699350197673986', 'reply_count': 570}\n",
      "{'conversation_id': '1266783358731931648', 'reply_count': 522}\n",
      "{'conversation_id': '1269010708886360066', 'reply_count': 261}\n",
      "{'conversation_id': '1283068355000373250', 'reply_count': 226}\n",
      "{'conversation_id': '1322871949815676930', 'reply_count': 197}\n",
      "{'conversation_id': '1384218399333511169', 'reply_count': 176}\n",
      "{'conversation_id': '1312127989363040258', 'reply_count': 171}\n",
      "{'conversation_id': '1314436759489400832', 'reply_count': 166}\n",
      "{'conversation_id': '1273988360286146561', 'reply_count': 151}\n",
      "{'conversation_id': '1279598008426954752', 'reply_count': 135}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for conv in get_convs(get_conv_events(Path('/home/ubuntu/jupyter/data/json.terms.files/prison_pipe_achievement_gap.json'))):\n",
    "    print(conv)\n",
    "    \n",
    "    count += 1 \n",
    "    if count > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c4576-cfce-4789-a9fa-d182c20c8cda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process the JSON files\n",
    "\n",
    "Now we need to get the JSON files and process each one! We can write the counts data alongside the tweets they came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1dbb7-8122-4e54-8753-1e7d58c6d2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_ppl.json had 94664 conversations\n",
      "prison_pipe_achievement_gap.json had 12920 conversations\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('/home/ubuntu/jupyter/data/json.terms.files')\n",
    "for path in data_dir.iterdir():\n",
    "\n",
    "    # ignore the convs files that we are generating\n",
    "    if path.suffix == '.json' and 'convs' not in path.name:\n",
    "        results = get_convs(get_conv_events(path))\n",
    "        convs_path = path.as_posix().replace('.json', '-convs.json')\n",
    "        json.dump(results, open(convs_path, 'w'), indent=2)\n",
    "        print(f'{path.name} had {len(results)} conversations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587b336-d947-4c69-a916-92473b06aa75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
