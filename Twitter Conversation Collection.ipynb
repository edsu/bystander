{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e389e3-c058-4328-b0cb-3d01c4717e67",
   "metadata": {},
   "source": [
    "# Twitter Conversation Collection\n",
    "\n",
    "This notebook analyzes previously collected tweets to identify *conversations* of interest, and to collect the conversations more throughly for closer analysis. In a previous set of notebooks stored in the s3://api.pull.code notebook a series of tweet JSON datasets were created using keyword searching and the Twitter Search API. These datasets were stored in the `s3://json.terms.files` bucket as a set of files: one file per search criteria. With the s3://json.terms.files bucket mounted this notebook will walk through the tweets and identify chatty conversations.\n",
    "\n",
    "## Chattiness\n",
    "\n",
    "This research is specifically about *bystander interventions* in social media. Part of the argument here is that a bystander intervention in public Twitter manifests as a conversation thread where two (or more) users are engaged in what looks like a conversation. In order to identifiy these threads a measure of *chattiness* will be generated using two pieces of information available in each tweet:\n",
    "\n",
    "* the `.public_metrics.reply_count` value available for each tweet, which is count of how many times a tweet has been replied to\n",
    "* the `.conversation_id` value which is an identifier for each threaded conversation\n",
    "\n",
    "In practice the number of users who participate in a thread is important too (a bystander intervention can't just be a user creating their own thread with no interaction). But we won't be able to ascertain that until we fetch the complete thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61546e67-b61a-4d43-adbe-50899cf6d38c",
   "metadata": {},
   "source": [
    "## Conversation Events\n",
    "\n",
    "The first step is to extract *conversation events* from the data retrieved from the Twitter Search API. `get_conversation_events()` takes a Path as an argument, and generates a conversation activity objects, each represented as a dictionary with the following keys: `author_id`, `conversation_id`, and `reply_count`. It filters out any retweets, which are important signals, but are not directly relevant to identifying conversation threads and bystander interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ce3efa-cce9-4269-82de-a4cb225668e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_conv_events(tweets_file):\n",
    "    \n",
    "    # parse each line of json\n",
    "    for line in tweets_file.open():\n",
    "        response = json.loads(line)\n",
    "        \n",
    "        # some reponses don't have data sometimes\n",
    "        if 'data' not in response:\n",
    "            continue\n",
    "        \n",
    "        # iterate through each tweets and yield any conversation info\n",
    "        for tweet in response['data']:      \n",
    "            \n",
    "            # ignore retweets\n",
    "            if 'retweeted' in [ref['type'] for ref in tweet.get('referenced_tweets', [])]:\n",
    "                continue\n",
    "            \n",
    "            # if the tweet has been replied to it's an event!\n",
    "            if tweet['public_metrics']['reply_count'] > 0:\n",
    "                yield({\n",
    "                    'conversation_id': tweet['conversation_id'],\n",
    "                    'reply_count': tweet['public_metrics']['reply_count']\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cf928-1a2d-4095-b9e2-37c31ce64d6d",
   "metadata": {},
   "source": [
    "Lets test it out just looking at the first 10 or so results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dcc767d-e020-4c96-b89c-f7deda975eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_id': '1396979032340717571', 'reply_count': 3}\n",
      "{'conversation_id': '1396968282972905476', 'reply_count': 1}\n",
      "{'conversation_id': '1396958637080391680', 'reply_count': 1}\n",
      "{'conversation_id': '1396953594616717316', 'reply_count': 1}\n",
      "{'conversation_id': '1396951539785285635', 'reply_count': 4}\n",
      "{'conversation_id': '1396950852452167680', 'reply_count': 1}\n",
      "{'conversation_id': '1396950478324371457', 'reply_count': 1}\n",
      "{'conversation_id': '1396938780330938368', 'reply_count': 19}\n",
      "{'conversation_id': '1396866492625563651', 'reply_count': 1}\n",
      "{'conversation_id': '1396866900236423170', 'reply_count': 1}\n",
      "{'conversation_id': '1396861451776708610', 'reply_count': 4}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "count = 0\n",
    "for event in get_conv_events(Path('/home/ubuntu/jupyter/data/json.terms.files/prison_pipe_achievement_gap.json')):\n",
    "    print(event)\n",
    "    \n",
    "    # stop after 10\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43cb2c-8520-4348-b740-476c63e2b344",
   "metadata": {},
   "source": [
    "## Aggregate Conversations\n",
    "\n",
    "Next we need to aggregate the conversations by ID. `get_convs()` reads in the conversation events and generates a list of conversations that includes their: `conversation_id` and total `reply_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25a46c5-e9e6-42b8-a6c9-fdfd19c3bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convs(events):\n",
    "    convos = {}\n",
    "    for e in events:\n",
    "        conv_id = e['conversation_id']\n",
    "        if conv_id in convos:\n",
    "            convos[conv_id]['reply_count'] += e['reply_count']\n",
    "        else:\n",
    "            convos[conv_id] = {\n",
    "                'conversation_id': conv_id,\n",
    "                'reply_count': e['reply_count'],\n",
    "            }\n",
    "    \n",
    "    # return the sorted conversations\n",
    "    convos = convos.values()\n",
    "    return sorted(convos, key=lambda c: c['reply_count'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c86d52-43bb-4dbd-ba06-10452b42a579",
   "metadata": {},
   "source": [
    "We can test this one too, by looking at the first 10 conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d111e5-0ca3-44fb-9eed-fc297e765c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_id': '1271571630045696001', 'reply_count': 735}\n",
      "{'conversation_id': '1289699350197673986', 'reply_count': 570}\n",
      "{'conversation_id': '1266783358731931648', 'reply_count': 522}\n",
      "{'conversation_id': '1269010708886360066', 'reply_count': 261}\n",
      "{'conversation_id': '1283068355000373250', 'reply_count': 226}\n",
      "{'conversation_id': '1322871949815676930', 'reply_count': 197}\n",
      "{'conversation_id': '1384218399333511169', 'reply_count': 176}\n",
      "{'conversation_id': '1312127989363040258', 'reply_count': 171}\n",
      "{'conversation_id': '1314436759489400832', 'reply_count': 166}\n",
      "{'conversation_id': '1273988360286146561', 'reply_count': 151}\n",
      "{'conversation_id': '1279598008426954752', 'reply_count': 135}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for conv in get_convs(get_conv_events(Path('/home/ubuntu/jupyter/data/json.terms.files/prison_pipe_achievement_gap.json'))):\n",
    "    print(conv)\n",
    "    \n",
    "    count += 1 \n",
    "    if count > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c4576-cfce-4789-a9fa-d182c20c8cda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract all the Conversations\n",
    "\n",
    "Now we need to get the JSON files and process each one! We can write the counts data alongside the tweets they came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3fbdb91-c7ab-4b7b-a976-2a9a471a2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu/jupyter/data/json.terms.files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c1dbb7-8122-4e54-8753-1e7d58c6d2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_ppl.json had 231400 conversations\n",
      "prison_pipe_achievement_gap.json had 12920 conversations\n",
      "black_us.json had 87058 conversations\n",
      "crime.json had 133919 conversations\n",
      "police_100.json had 79 conversations\n",
      "wealth.json had 8725 conversations\n",
      "police_violence.json had 43243 conversations\n",
      "police.json had 123869 conversations\n",
      "business.json had 81583 conversations\n",
      "floyd_chauvin.json had 46533 conversations\n",
      "blm.json had 215291 conversations\n",
      "racism.json had 214756 conversations\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for path in data_dir.iterdir():\n",
    "\n",
    "    # ignore the convs files that we are generating\n",
    "    if path.suffix == '.json' and '_convs' not in path.name:\n",
    "        results = get_convs(get_conv_events(path))\n",
    "        convs_path = path.as_posix().replace('.json', '_convs.json')\n",
    "        json.dump(results, open(convs_path, 'w'), indent=2)\n",
    "        print(f'{path.name} had {len(results)} conversations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39157797-5311-4170-9a0e-93a1170fcc89",
   "metadata": {},
   "source": [
    "## Getting the Conversations\n",
    "\n",
    "So what do these conversations look like? That's really the subject for another notebook, as this one is concerned with *collecting* the conversations. But we do have one more step to fetch each of the conversations. All we have are pieces of conversations that came back from our searches, and pointers to some of those threads.\n",
    "\n",
    "Fortunately the Twitter APIs now supports searching for tweets using their `conversation_id`. This allows the complete conversation thread to be fetched. This next bit of code gets the top 100 conversations for each dataset, and writes the full conversation thread as JSON and as CSV to a directory named after the dataset. Having the data as CSV should help when analyzing the threads in other tools.\n",
    "\n",
    "To fetch data from the Twitter API you will need to have previously run `twarc2 configure` in the environment where this notebook is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87a38d-52dc-48c7-89e0-305d2d74243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sh import twarc2\n",
    "\n",
    "for conv_file in data_dir.glob('*convs.json'):\n",
    "    convs = json.load(open(conv_file))        \n",
    "    print(f'processing {conv_file}')\n",
    "\n",
    "    conv_dir = data_dir / conv_file.name.replace('_convs.json', '_convs')\n",
    "    if not conv_dir.is_dir():\n",
    "        conv_dir.mkdir()\n",
    "    \n",
    "    # get the full threads for the top 100 conversation ids\n",
    "    for conv in convs[0:100]:\n",
    "        print(conv)\n",
    "        conv_id = conv['conversation_id']\n",
    "        conv_json = conv_dir / f'{conv_id}.json' \n",
    "        conv_csv = conv_dir / f'{conv_id}.csv'\n",
    "        \n",
    "        # don't re-generate the csv if we already have it!\n",
    "        if conv_csv.is_file():\n",
    "            continue\n",
    "\n",
    "        # get the json, convert to csv and remove the json\n",
    "        twarc2('conversation', '--archive', conv_id, conv_json)\n",
    "        \n",
    "        # sometimes there is nothing to retrieve for the conversation_id\n",
    "        if conv_json.is_file():\n",
    "            twarc2('csv', conv_json, conv_csv)\n",
    "            os.remove(conv_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a7e4d-bd40-4d96-9e30-8c2e9087a4e0",
   "metadata": {},
   "source": [
    "## Random Sample\n",
    "\n",
    "In addition to getting the top 100 conversations for each tweet dataset we can get a random sample of all the conversations, and save them as CSV for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af2a7b1f-a3b7-4c6e-9c54-bd4521655357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sh\n",
    "import pandas\n",
    "\n",
    "seen = {}\n",
    "twitter_json_dir = Path('/home/ubuntu/jupyter/data/tweets.pull')\n",
    "\n",
    "def sample(convs_file, n):\n",
    "    convs_dir = twitter_json_dir / (convs_file.stem + f\"_{n}\")\n",
    "    if not convs_dir.is_dir():\n",
    "        convs_dir.mkdir()\n",
    "\n",
    "    convs = json.load(open(convs_file))\n",
    "    df = pandas.DataFrame(convs)\n",
    "    \n",
    "    # sample size cannot be bigger than the dataframe\n",
    "    if n > len(df):\n",
    "        n = len(df)\n",
    "\n",
    "    s = df.sample(n)\n",
    "    \n",
    "    for conv_id in s[\"conversation_id\"]:\n",
    "        conv_json = convs_dir / f\"{conv_id}.jsonl\"\n",
    "        conv_csv = convs_dir / f\"{conv_id}.csv\"\n",
    "        if conv_id in seen:\n",
    "            print(f\"using already fetched {conv_csv}\")\n",
    "            sh.cp(seen[conv_id], conv_csv)\n",
    "        else:\n",
    "            print(conv_csv)\n",
    "            sh.twarc2(\"conversation\", \"--archive\", conv_id, conv_json)\n",
    "            # if the conversation_id no longer yields any tweets fhe json file will not exist\n",
    "            if not conv_json.is_file():\n",
    "                print(f\"conversation {conv_id} no longer exists\")\n",
    "                sh.touch(conv_json)\n",
    "            else:\n",
    "                sh.twarc2(\"csv\", conv_json, conv_csv)\n",
    "                sh.rm(conv_json)\n",
    "                seen[conv_id] = conv_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6637a-8165-4a13-b243-eba81172a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "for convs_file in data_dir.glob(\"*convs.json\"):\n",
    "    sample(convs_file, 30)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
