{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ef0a3f-f8b2-4afa-93a7-f61c3959273e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "We anticipate that (despite its known shortcomings) sentiment analysis could provide an interesting view into our conversation data. This notebook takes the random samples of Twitter and Reddit conversations that were previously generated and will add sentiment scores using [Vader](https://github.com/cjhutto/vaderSentiment). See Melanie Walsh's description of Vader in [this Jupyter notebook](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html) for more context on the approach that was taken here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d1d53b2-838d-47f3-bf8f-d3ed21a93d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.647, 'neu': 0.353, 'pos': 0.0, 'compound': -0.6705}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "vader.polarity_scores(\"This is a terrible mistake.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d106b1-9a3e-4c2b-8df7-bacd703fe47a",
   "metadata": {},
   "source": [
    "As Walsh suggests we are going to be using the `compound` value as an aggregate of `negative`, `neutral` and `positive` scores. The Twitter and Reddit conversations that have been sampled have been put into a zip file per original dataset. So for the \"racism\" tweets there is a zip file containing 30 CSV files, each of which contains metadata for replies in a particular thread that was found in the Twitter search results for \"racism\". Similarly for the \"racism\" Reddit posts there is a zip file containing 30 CSV files, each of which contains metadata for comments in a particular thread found in the Reddit search results for \"racism\".\n",
    "\n",
    "This notebook will unpack thosze zip files, add the sentiment scores, and then zip them back up again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08abfe4-db53-4f14-afb8-e4304904efb4",
   "metadata": {},
   "source": [
    "## Twitter\n",
    "\n",
    "While working with the zip files it's easiest if we change directory to where the zip files are.\n",
    "\n",
    "But we can write a function that will take a zip file, and a column name to use for the text, which will read each csv in the zip file, write out the csv with a new *sentiment* column, and then zip it up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9d58082-2841-4963-9abf-217b2082fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sh\n",
    "import pandas\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "tweets_dir = Path('/home/ubuntu/jupyter/data/tweets.pull')\n",
    "os.chdir(tweets_dir)\n",
    "\n",
    "def process(zipfile, text_col):\n",
    "    sh.unzip('-u', zipfile)\n",
    "    sample_dir = Path(zipfile.stem)\n",
    "    for csv_file in sample_dir.glob('*.csv'):\n",
    "        print(csv_file)\n",
    "        df = pandas.read_csv(csv_file)\n",
    "        df['sentiment'] = df[text_col].apply(lambda t: vader.polarity_scores(t)['compound'])\n",
    "        df.to_csv(csv_file, index=False)\n",
    "    sh.zip(zipfile.name, sample_dir.name)\n",
    "    sh.rm('-rf', sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bd335-44c3-4946-a0ac-99657c06d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zipfile in tweets_dir.glob('*_30.zip'):\n",
    "    print(zipfile)\n",
    "    process(zipfile, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b49f70-296f-4bbe-a13f-1fd48367cdbb",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "164dfb78-424d-4e54-be93-e3fa4b0f3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jupyter/data/reddit.pull/reddit_black_people_convs_30.zip\n",
      "reddit_black_people_convs_30/lspdve.csv\n",
      "reddit_black_people_convs_30/hc1qww.csv\n",
      "reddit_black_people_convs_30/irr3vx.csv\n",
      "reddit_black_people_convs_30/it2tgn.csv\n",
      "reddit_black_people_convs_30/iu73ac.csv\n",
      "reddit_black_people_convs_30/i6et5p.csv\n",
      "reddit_black_people_convs_30/kumrlb.csv\n",
      "reddit_black_people_convs_30/i4rsdb.csv\n",
      "reddit_black_people_convs_30/huknvk.csv\n",
      "reddit_black_people_convs_30/ihixet.csv\n",
      "reddit_black_people_convs_30/gye49e.csv\n",
      "reddit_black_people_convs_30/hqqpr8.csv\n",
      "reddit_black_people_convs_30/n5c6rl.csv\n",
      "reddit_black_people_convs_30/hd4266.csv\n",
      "reddit_black_people_convs_30/gwfghb.csv\n",
      "reddit_black_people_convs_30/ir7jei.csv\n",
      "reddit_black_people_convs_30/lppo6o.csv\n",
      "reddit_black_people_convs_30/gw55oe.csv\n",
      "reddit_black_people_convs_30/ifn3o9.csv\n",
      "reddit_black_people_convs_30/i4tskq.csv\n",
      "reddit_black_people_convs_30/n0t5jw.csv\n",
      "reddit_black_people_convs_30/ifa34n.csv\n",
      "reddit_black_people_convs_30/iryadd.csv\n",
      "reddit_black_people_convs_30/neybzu.csv\n",
      "reddit_black_people_convs_30/ncgbu8.csv\n",
      "reddit_black_people_convs_30/ha3hql.csv\n",
      "reddit_black_people_convs_30/m2mpih.csv\n",
      "reddit_black_people_convs_30/jkm3ee.csv\n",
      "reddit_black_people_convs_30/htf77j.csv\n",
      "reddit_black_people_convs_30/n4go9x.csv\n",
      "/home/ubuntu/jupyter/data/reddit.pull/reddit_racial_wealth_gap_convs_30.zip\n",
      "reddit_racial_wealth_gap_convs_30/m4ljb0.csv\n",
      "reddit_racial_wealth_gap_convs_30/ifotnt.csv\n",
      "reddit_racial_wealth_gap_convs_30/jwh0qy.csv\n",
      "reddit_racial_wealth_gap_convs_30/kswcrt.csv\n",
      "reddit_racial_wealth_gap_convs_30/kg1vjx.csv\n",
      "reddit_racial_wealth_gap_convs_30/hbwbcl.csv\n",
      "reddit_racial_wealth_gap_convs_30/ht2qiq.csv\n",
      "reddit_racial_wealth_gap_convs_30/hggqkb.csv\n",
      "reddit_racial_wealth_gap_convs_30/hwdozb.csv\n",
      "reddit_racial_wealth_gap_convs_30/mz4jax.csv\n",
      "reddit_racial_wealth_gap_convs_30/hwexm4.csv\n",
      "reddit_racial_wealth_gap_convs_30/jq41ar.csv\n",
      "reddit_racial_wealth_gap_convs_30/gvwp12.csv\n",
      "reddit_racial_wealth_gap_convs_30/hibs7v.csv\n",
      "reddit_racial_wealth_gap_convs_30/mi4x7j.csv\n",
      "reddit_racial_wealth_gap_convs_30/ioq56v.csv\n",
      "reddit_racial_wealth_gap_convs_30/n07vzw.csv\n",
      "reddit_racial_wealth_gap_convs_30/ho9wg9.csv\n",
      "reddit_racial_wealth_gap_convs_30/j84ala.csv\n",
      "reddit_racial_wealth_gap_convs_30/jmymqo.csv\n",
      "reddit_racial_wealth_gap_convs_30/m58eh0.csv\n",
      "reddit_racial_wealth_gap_convs_30/kc94um.csv\n",
      "reddit_racial_wealth_gap_convs_30/mrgfif.csv\n",
      "reddit_racial_wealth_gap_convs_30/j7c5r4.csv\n",
      "reddit_racial_wealth_gap_convs_30/ng3wy1.csv\n",
      "reddit_racial_wealth_gap_convs_30/i004fs.csv\n",
      "reddit_racial_wealth_gap_convs_30/jlmu8v.csv\n",
      "reddit_racial_wealth_gap_convs_30/iwhfas.csv\n",
      "reddit_racial_wealth_gap_convs_30/lahmim.csv\n",
      "reddit_racial_wealth_gap_convs_30/ifccra.csv\n",
      "/home/ubuntu/jupyter/data/reddit.pull/reddit_racism_convs_30.zip\n",
      "reddit_racism_convs_30/njou2o.csv\n",
      "reddit_racism_convs_30/m5l1oz.csv\n",
      "reddit_racism_convs_30/k9red6.csv\n",
      "reddit_racism_convs_30/jrx925.csv\n",
      "reddit_racism_convs_30/mhcyd1.csv\n",
      "reddit_racism_convs_30/ls8hb8.csv\n",
      "reddit_racism_convs_30/likom8.csv\n",
      "reddit_racism_convs_30/kynqzk.csv\n",
      "reddit_racism_convs_30/idu5az.csv\n",
      "reddit_racism_convs_30/hzh32r.csv\n",
      "reddit_racism_convs_30/mrixcj.csv\n",
      "reddit_racism_convs_30/lqbvh4.csv\n",
      "reddit_racism_convs_30/mrep51.csv\n",
      "reddit_racism_convs_30/m3awqu.csv\n",
      "reddit_racism_convs_30/long0f.csv\n",
      "reddit_racism_convs_30/hfxbdy.csv\n",
      "reddit_racism_convs_30/l9ytbo.csv\n",
      "reddit_racism_convs_30/iqg225.csv\n",
      "reddit_racism_convs_30/n2j8tn.csv\n",
      "reddit_racism_convs_30/liy8g7.csv\n",
      "reddit_racism_convs_30/hv3cmo.csv\n",
      "reddit_racism_convs_30/hqpgl3.csv\n",
      "reddit_racism_convs_30/hw6fu1.csv\n",
      "reddit_racism_convs_30/hgpdea.csv\n",
      "reddit_racism_convs_30/gqp3ci.csv\n",
      "reddit_racism_convs_30/l9xyer.csv\n",
      "reddit_racism_convs_30/iieg5f.csv\n",
      "reddit_racism_convs_30/ju7xcq.csv\n",
      "reddit_racism_convs_30/hgcg1s.csv\n",
      "reddit_racism_convs_30/hqxwef.csv\n"
     ]
    }
   ],
   "source": [
    "reddit_dir = Path('/home/ubuntu/jupyter/data/reddit.pull')\n",
    "os.chdir(reddit_dir)\n",
    "\n",
    "for zipfile in reddit_dir.glob('*_30.zip'):\n",
    "    print(zipfile)\n",
    "    process(zipfile, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fbe283-ffc3-42a7-a9d8-c9b54606c50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
